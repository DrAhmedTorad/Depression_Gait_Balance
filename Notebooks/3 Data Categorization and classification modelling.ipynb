{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qk36QUoDycO"
      },
      "source": [
        "# Predection of Depression by different gait and Balance parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTAqggVyDycW"
      },
      "source": [
        "# Import Data From File"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "JwKaL5ZMFV6N",
        "outputId": "7a523ef1-94f2-4b82-e864-4b1dc3cb5571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aba3cfce-6aad-42e4-8390-a40e052a0507\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aba3cfce-6aad-42e4-8390-a40e052a0507\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving All_Data_After_Imputation_Winsoring_Classification.xlsx to All_Data_After_Imputation_Winsoring_Classification (2).xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Sqe9HJx-DycY"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "# Path of Raw Data on the computer\n",
        "DATA_PATH = r'All_Data_After_Imputation_Winsoring_Classification.xlsx'\n",
        "# import DATA after having the LABLE (Frist Columen is Y) columnes 2 and 3 are the neuomerical data of predectors form thim Y calculated)\n",
        "df_imput_winsor = pd.read_excel(DATA_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_imput_winsor = df_imput_winsor[df_imput_winsor['Age_A'] <= 36]\n",
        "df_imput_winsor = df_imput_winsor.reset_index()"
      ],
      "metadata": {
        "id": "G9nzO5Y7tWSh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having fixed variables alone (Y variables)\n",
        "df_imput_winsor_scal_X = df_imput_winsor.drop(['CurrentPOMSdepression', 'Sex'], axis=1)\n",
        "# Store the removed variables to be attached at the end of cleaning prosses\n",
        "cat_data = df_imput_winsor[['CurrentPOMSdepression', 'Sex']]\n",
        "# Apply Stander Scaller for the DataFrame (Normalization)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "# # perform a robust scaler transform of the dataset\n",
        "trans = Normalizer()\n",
        "X_imput_winsor_Scaled = trans.fit_transform(df_imput_winsor_scal_X)\n",
        "# convert the array back to a dataframe\n",
        "X_imput_winsor_Scaled = pd.DataFrame(X_imput_winsor_Scaled)\n",
        "X_imput_winsor_Scaled.columns = df_imput_winsor_scal_X.columns\n",
        "# Concatenate Y variabels with X_imput_scaled features in ONE data farme\n",
        "df_imput_winsor_scal = pd.concat([cat_data,X_imput_winsor_Scaled], axis=1)"
      ],
      "metadata": {
        "id": "DklcyPuPuqC7",
        "outputId": "90b9a77e-f1bd-43de-ed26-68356cbb67fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-6292d62530f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Having fixed variables alone (Y variables)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_imput_winsor_scal_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_imput_winsor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CurrentPOMSdepression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Store the removed variables to be attached at the end of cleaning prosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_imput_winsor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CurrentPOMSdepression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Apply Stander Scaller for the DataFrame (Normalization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4911\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4912\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4913\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4914\u001b[0m         )\n\u001b[1;32m   4915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['CurrentPOMSdepression'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df_imput_winsor_scal "
      ],
      "metadata": {
        "id": "sMPpAdOmurZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiCLwkAhDycb"
      },
      "source": [
        "# Variable Categorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-WDFZBzDDycc"
      },
      "outputs": [],
      "source": [
        "# Y Variables\n",
        "Y_Var = ['CurrentPOMSdepressionClasses']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U9nb1vdDDycc"
      },
      "outputs": [],
      "source": [
        "# Demographic_variables\n",
        "Demographic_var = ['Sex', 'Age', 'Heightcm', 'WeightinKG', 'CalculatedBMI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VZwZ34rvDycd"
      },
      "outputs": [],
      "source": [
        "# Gait_Variables\n",
        "ANTICEPATORY_POSTURE = ['AnticipatoryPosturalAdjustmentAPADurations','AnticipatoryPosturalAdjustmentFirstStepDurations','AnticipatoryPosturalAdjustmentFirstStepRangeofMotiondegrees','AnticipatoryPosturalAdjustmentForwardAPAPeakms2','AnticipatoryPosturalAdjustmentLateralAPAPeakms2']\n",
        "BACK_ROM = ['GaitJointBackRightLat.BendMax.degreesmean','GaitJointBackRightLat.BendMax.degreesstd','GaitJointBackLeftLat.BendMax.degreesmean','GaitJointBackLeftLat.BendMax.degreesstd','GaitJointBackRLLatBendRangedegreesmean','GaitJointBackRLLatBendRangedegreesstd','GaitJointBackFlexExtMax.degreesmean','GaitJointBackFlexExtMax.degreesstd','GaitJointBackFlexExtMin.degreesmean','GaitJointBackFlexExtMin.degreesstd','GaitJointBackFlexExtRangedegreesmean','GaitJointBackFlexExtRangedegreesstd','GaitJointBackRightRotMax.degreesmean','GaitJointBackRightRotMax.degreesstd','GaitJointBackLeftRotMax.degreesmean','GaitJointBackLeftRotMax.degreesstd','GaitJointBackRLRotRangedegreesmean','GaitJointBackRLRotRangedegreesstd']\n",
        "NECK_ROM = ['GaitJointNeckRightLat.BendMax.degreesmean','GaitJointNeckRightLat.BendMax.degreesstd','GaitJointNeckLeftLat.BendMax.degreesmean','GaitJointNeckLeftLat.BendMax.degreesstd','GaitJointNeckRLLat.BendRangedegreesmean','GaitJointNeckRLLat.BendRangedegreesstd','GaitJointNeckFlexExtMax.degreesmean','GaitJointNeckFlexExtMax.degreesstd','GaitJointNeckFlexExtMin.degreesmean','GaitJointNeckFlexExtMin.degreesstd','GaitJointNeckFlexExtRangedegreesmean','GaitJointNeckFlexExtRangedegreesstd','GaitJointNeckRightRotMax.degreesmean','GaitJointNeckRightRotMax.degreesstd','GaitJointNeckLeftRotMax.degreesmean','GaitJointNeckLeftRotMax.degreesstd','GaitJointNeckRLRotRangedegreesmean','GaitJointNeckRLRotRangedegreesstd']\n",
        "CADENCE = ['GaitLowerLimbCadenceLstepsminmean','GaitLowerLimbCadenceLstepsminstd','GaitLowerLimbCadenceRstepsminmean','GaitLowerLimbCadenceRstepsminstd','Avgcadence','Cadenceasymmetry']\n",
        "DOUBLE_LIMB_SUPPORT_GCT = ['GaitLowerLimbDoubleSupportLGCTmean','GaitLowerLimbDoubleSupportLGCTstd','GaitLowerLimbDoubleSupportRGCTmean','GaitLowerLimbDoubleSupportRGCTstd','AvgdoublelegsupportofGCT','AsymetriesdoublelegsupportGCT']\n",
        "MIDSWING = ['GaitLowerLimbElevationatMidswingLcmmean','GaitLowerLimbElevationatMidswingLcmstd','GaitLowerLimbElevationatMidswingRcmmean','GaitLowerLimbElevationatMidswingRcmstd','Avgmidswingelevation','Asymetrymidswingelevation']\n",
        "GAIT_CYCLE_DURATION = ['GaitLowerLimbGaitCycleDurationLsmean','GaitLowerLimbGaitCycleDurationLsstd','GaitLowerLimbGaitCycleDurationRsmean','GaitLowerLimbGaitCycleDurationRsstd','Avggaitcycleduration','Asymmetrygaitcycleduration']\n",
        "GAIT_SPEED = ['GaitLowerLimbGaitSpeedLmsmean','GaitLowerLimbGaitSpeedLmsstd','GaitLowerLimbGaitSpeedRmsmean','GaitLowerLimbGaitSpeedRmsstd','Avggaitspeed','Asymmetrygaitspeed']\n",
        "GAIT_LATERAL_VARIATION = ['GaitLowerLimbLateralStepVariabilityLcm','GaitLowerLimbLateralStepVariabilityRcm','Avgstepvariability','Asymmetrystepvariability']\n",
        "CIRCUMDUCTION_GAIT = ['GaitLowerLimbCircumductionLcmmean','GaitLowerLimbCircumductionLcmstd','GaitLowerLimbCircumductionRcmmean','GaitLowerLimbCircumductionRcmstd','Avgcircumduction','Asymmetrycircumdunction']\n",
        "FOOT_STRIKE = ['GaitLowerLimbFootStrikeAngleLdegreesmean','GaitLowerLimbFootStrikeAngleLdegreesstd','GaitLowerLimbFootStrikeAngleRdegreesmean','GaitLowerLimbFootStrikeAngleRdegreesstd','Avgfootstrikeangle','Asymmetryfootstrikeangle']\n",
        "TOE_OFF = ['GaitLowerLimbToeOffAngleLdegreesmean','GaitLowerLimbToeOffAngleLdegreesstd','GaitLowerLimbToeOffAngleRdegreesmean','GaitLowerLimbToeOffAngleRdegreesstd','AvgToeoutangle','AsymmetryToeoutangle']\n",
        "SINGLE_LIMB_SUPPORT = ['GaitLowerLimbSingleLimbSupportLGCTmean','GaitLowerLimbSingleLimbSupportLGCTstd','GaitLowerLimbSingleLimbSupportRGCTmean','GaitLowerLimbSingleLimbSupportRGCTstd','AvgSinglelegsupportofGCT','AsymmetrySinglelegsupportofGCT']\n",
        "LIMB_STANCE = ['GaitLowerLimbStanceLGCTmean','GaitLowerLimbStanceLGCTstd','GaitLowerLimbStanceRGCTmean','GaitLowerLimbStanceRGCTstd','AvgStanceofGCT','AsymmetrystanceofGCT']\n",
        "STEP_DURATION = ['GaitLowerLimbStepDurationLsmean','GaitLowerLimbStepDurationLsstd','GaitLowerLimbStepDurationRsmean','GaitLowerLimbStepDurationRsstd','Avgstepduration','Asymmetrystepduration']\n",
        "STRIDE_LENGTH = ['GaitLowerLimbStrideLengthLmmean','GaitLowerLimbStrideLengthLmstd','GaitLowerLimbStrideLengthRmmean','GaitLowerLimbStrideLengthRmstd','Avgstridelength','Asymmetrystridelength']\n",
        "LIMB_SWING = ['GaitLowerLimbSwingLGCTmean','GaitLowerLimbSwingLGCTstd','GaitLowerLimbSwingRGCTmean','GaitLowerLimbSwingRGCTstd','AvgswingofGCT','AsymmetryswingofGCT']\n",
        "TERMINAL_DOUBLE_LIMB_SUPPORT_GCT = ['GaitLowerLimbTerminalDoubleSupportLGCTmean','GaitLowerLimbTerminalDoubleSupportLGCTstd','GaitLowerLimbTerminalDoubleSupportRGCTmean','GaitLowerLimbTerminalDoubleSupportRGCTstd','AvgterminaldoublelegsupportofGCT','AsymmetryterminaldoublelegsupportofGCT']\n",
        "TOE_OUT_ANGLE = ['GaitLowerLimbToeOutAngleLdegreesmean','GaitLowerLimbToeOutAngleLdegreesstd','GaitLowerLimbToeOutAngleRdegreesmean','GaitLowerLimbToeOutAngleRdegreesstd','AvgToeoutangle_A','AsymmetryAvgToeoutangle']\n",
        "LUMBER_ROM_IN_PLANES = ['GaitLumbarCoronalRangeofMotiondegreesmean','GaitLumbarCoronalRangeofMotiondegreesstd','GaitLumbarSagittalRangeofMotiondegreesmean','GaitLumbarSagittalRangeofMotiondegreesstd','GaitLumbarTransverseRangeofMotiondegreesmean','GaitLumbarTransverseRangeofMotiondegreesstd']\n",
        "TRUNK_POM_IN_PLANES = ['GaitTrunkCoronalRangeofMotiondegreesmean','GaitTrunkCoronalRangeofMotiondegreesstd','GaitTrunkSagittalRangeofMotiondegreesmean','GaitTrunkSagittalRangeofMotiondegreesstd','GaitTrunkTransverseRangeofMotiondegreesmean','GaitTrunkTransverseRangeofMotiondegreesstd','GaitUpperLimbArmSwingVelocityLdegreessmean','GaitUpperLimbArmSwingVelocityLdegreessstd','GaitUpperLimbArmSwingVelocityRdegreessmean','GaitUpperLimbArmSwingVelocityRdegreessstd','AvgUpperArmswingvelocity','Asymmetryupperarmswingvelocity']\n",
        "UPPER_LIMBS_ROM = ['GaitUpperLimbArmRangeofMotionLdegreesmean','GaitUpperLimbArmRangeofMotionLdegreesstd','GaitUpperLimbArmRangeofMotionRdegreesmean','GaitUpperLimbArmRangeofMotionRdegreesstd','AvgupperarmROM','AsymmetryupperarmROM']\n",
        "TURNS = ['TurnsAngledegreesmean','TurnsAngledegreesstd','TurnsDurationsmean','TurnsDurationsstd','TurnsN#','TurnsTurnVelocitydegreessmean','TurnsTurnVelocitydegreessstd','TurnsStepsinTurn#mean','TurnsStepsinTurn#std']\n",
        "Gait_SUBCATIGEROIES = ANTICEPATORY_POSTURE + BACK_ROM + NECK_ROM + CADENCE + DOUBLE_LIMB_SUPPORT_GCT + MIDSWING + GAIT_CYCLE_DURATION + GAIT_SPEED + GAIT_LATERAL_VARIATION + CIRCUMDUCTION_GAIT + FOOT_STRIKE + TOE_OFF + SINGLE_LIMB_SUPPORT + LIMB_STANCE + STEP_DURATION + STRIDE_LENGTH + LIMB_SWING + TERMINAL_DOUBLE_LIMB_SUPPORT_GCT + TOE_OUT_ANGLE + LUMBER_ROM_IN_PLANES + TRUNK_POM_IN_PLANES + UPPER_LIMBS_ROM + TURNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Eq4kqxOyDycg"
      },
      "outputs": [],
      "source": [
        "# Balance_Conditions\n",
        "Condition1 = [\"PosturalSwayAcc95EllipseAxis1Radiusms2\",\"PosturalSwayAcc95EllipseAxis2Radiusms2\",\"PosturalSwayAcc95EllipseRotationms2\",\n",
        "\"PosturalSwayAcc95EllipseSwayAream2s4\",\"PosturalSwayAccCentroidalFrequencyHz\",\"PosturalSwayAccCentroidalFrequencyCoronalHz\",\n",
        "\"PosturalSwayAccCentroidalFrequencySagittalHz\",\"PosturalSwayAccFrequencyDispersionAD\",\"PosturalSwayAccFrequencyDispersionCoronalAD\",\n",
        "\"PosturalSwayAccFrequencyDispersionSagittalAD\",\"PosturalSwayAccJerkm2s5\",\"PosturalSwayAccJerkCoronalm2s5\",\n",
        "\"PosturalSwayAccJerkSagittalm2s5\",\"PosturalSwayAccMeanVelocityms\",\"PosturalSwayAccMeanVelocityCoronalms\",\n",
        "\"PosturalSwayAccMeanVelocitySagittalms\",\"PosturalSwayAccPathLengthms2\",\"PosturalSwayAccPathLengthCoronalms2\",\n",
        "\"PosturalSwayAccPathLengthSagittalms2\",\"PosturalSwayAccRMSSwayms2\",\"PosturalSwayAccRMSSwayCoronalms2\",\n",
        "\"PosturalSwayAccRMSSwaySagittalms2\",\"PosturalSwayAccRangems2\",\"PosturalSwayAccRangeCoronalms2\",\"PosturalSwayAccRangeSagittalms2\",\n",
        "\"PosturalSwayAnglesSwayAreaRadiusCoronaldegrees\",\"PosturalSwayAngles95EllipseAxis2Radiusdegrees\",\n",
        "\"PosturalSwayAnglesSwayAreaRotationdegrees\", \"PosturalSwayAnglesSwayAreadegrees2\",\"PosturalSwayAnglesDurations\",\n",
        "\"PosturalSwayAnglesRMSSwaydegrees\",\"PosturalSwayAnglesRMSSwayCoronaldegrees\",\"PosturalSwayAnglesRMSSwaySagittaldegrees\"]\n",
        "Condition2 = [\"PosturalSwayAcc95EllipseAxis1Radiusms2_A\",\"PosturalSwayAcc95EllipseAxis2Radiusms2_A\",\"PosturalSwayAcc95EllipseRotationms2_A\",\n",
        "\"PosturalSwayAcc95EllipseSwayAream2s4_A\",\"PosturalSwayAccCentroidalFrequencyHz_A\",\"PosturalSwayAccCentroidalFrequencyCoronalHz_A\",\n",
        "\"PosturalSwayAccCentroidalFrequencySagittalHz_A\",\"PosturalSwayAccFrequencyDispersionAD_A\",\"PosturalSwayAccFrequencyDispersionCoronalAD_A\",\n",
        "\"PosturalSwayAccFrequencyDispersionSagittalAD_A\",\"PosturalSwayAccJerkm2s5_A\", \"PosturalSwayAccJerkCoronalm2s5_A\",\n",
        "\"PosturalSwayAccJerkSagittalm2s5_A\",\"PosturalSwayAccMeanVelocityms_A\",\"PosturalSwayAccMeanVelocityCoronalms_A\",\n",
        "\"PosturalSwayAccMeanVelocitySagittalms_A\",\"PosturalSwayAccPathLengthms2_A\",\"PosturalSwayAccPathLengthCoronalms2_A\",\n",
        "\"PosturalSwayAccPathLengthSagittalms2_A\",\"PosturalSwayAccRMSSwayms2_A\",\"PosturalSwayAccRMSSwayCoronalms2_A\",\n",
        "\"PosturalSwayAccRMSSwaySagittalms2_A\",\"PosturalSwayAccRangems2_A\",\"PosturalSwayAccRangeCoronalms2_A\",\n",
        "\"PosturalSwayAccRangeSagittalms2_A\",\"PosturalSwayAnglesSwayAreaRadiusCoronaldegrees_A\",\"PosturalSwayAngles95EllipseAxis2Radiusdegrees_A\",\n",
        "\"PosturalSwayAnglesSwayAreaRotationdegrees_A\",\"PosturalSwayAnglesSwayAreadegrees2_A\", \"PosturalSwayAnglesDurations_A\",\n",
        "\"PosturalSwayAnglesRMSSwaydegrees_A\",\"PosturalSwayAnglesRMSSwayCoronaldegrees_A\",\"PosturalSwayAnglesRMSSwaySagittaldegrees_A\"]\n",
        "Condition3 = [\"PosturalSwayAcc95EllipseAxis1Radiusms2_B\",\"PosturalSwayAcc95EllipseAxis2Radiusms2_B\", \"PosturalSwayAcc95EllipseRotationms2_B\",\n",
        "\"PosturalSwayAcc95EllipseSwayAream2s4_B\",\"PosturalSwayAccCentroidalFrequencyHz_B\",\"PosturalSwayAccCentroidalFrequencyCoronalHz_B\",\n",
        "\"PosturalSwayAccCentroidalFrequencySagittalHz_B\",\"PosturalSwayAccFrequencyDispersionAD_B\",\"PosturalSwayAccFrequencyDispersionCoronalAD_B\",\n",
        "\"PosturalSwayAccFrequencyDispersionSagittalAD_B\",\"PosturalSwayAccJerkm2s5_B\",\"PosturalSwayAccJerkCoronalm2s5_B\",\n",
        "\"PosturalSwayAccJerkSagittalm2s5_B\",\"PosturalSwayAccMeanVelocityms_B\",\"PosturalSwayAccMeanVelocityCoronalms_B\",\n",
        "\"PosturalSwayAccMeanVelocitySagittalms_B\",\"PosturalSwayAccPathLengthms2_B\",\"PosturalSwayAccPathLengthCoronalms2_B\",\n",
        "\"PosturalSwayAccPathLengthSagittalms2_B\",\"PosturalSwayAccRMSSwayms2_B\",\"PosturalSwayAccRMSSwayCoronalms2_B\",\n",
        "\"PosturalSwayAccRMSSwaySagittalms2_B\",\"PosturalSwayAccRangems2_B\",\"PosturalSwayAccRangeCoronalms2_B\",\n",
        "\"PosturalSwayAccRangeSagittalms2_B\",\"PosturalSwayAnglesSwayAreaRadiusCoronaldegrees_B\",\"PosturalSwayAngles95EllipseAxis2Radiusdegrees_B\",\n",
        "\"PosturalSwayAnglesSwayAreaRotationdegrees_B\",\"PosturalSwayAnglesSwayAreadegrees2_B\",\"PosturalSwayAnglesDurations_B\",\n",
        "\"PosturalSwayAnglesRMSSwaydegrees_B\",\"PosturalSwayAnglesRMSSwayCoronaldegrees_B\",\"PosturalSwayAnglesRMSSwaySagittaldegrees_B\"]\n",
        "Condition4 = [\"PosturalSwayAcc95EllipseAxis1Radiusms2_C\",\"PosturalSwayAcc95EllipseAxis2Radiusms2_C\",\n",
        "\"PosturalSwayAcc95EllipseRotationms2_C\",\"PosturalSwayAcc95EllipseSwayAream2s4_C\",\"PosturalSwayAccCentroidalFrequencyHz_C\",\n",
        "\"PosturalSwayAccCentroidalFrequencyCoronalHz_C\",\"PosturalSwayAccCentroidalFrequencySagittalHz_C\",\"PosturalSwayAccFrequencyDispersionAD_C\",\n",
        "\"PosturalSwayAccFrequencyDispersionCoronalAD_C\",\"PosturalSwayAccFrequencyDispersionSagittalAD_C\",\"PosturalSwayAccJerkm2s5_C\",\n",
        "\"PosturalSwayAccJerkCoronalm2s5_C\",\"PosturalSwayAccJerkSagittalm2s5_C\",\"PosturalSwayAccMeanVelocityms_C\",\n",
        "\"PosturalSwayAccMeanVelocityCoronalms_C\",\"PosturalSwayAccMeanVelocitySagittalms_C\",\n",
        "\"PosturalSwayAccPathLengthms2_C\",\"PosturalSwayAccPathLengthCoronalms2_C\",\"PosturalSwayAccPathLengthSagittalms2_C\",\n",
        "\"PosturalSwayAccRMSSwayms2_C\",\"PosturalSwayAccRMSSwayCoronalms2_C\",\"PosturalSwayAccRMSSwaySagittalms2_C\",\n",
        "\"PosturalSwayAccRangems2_C\",\"PosturalSwayAccRangeCoronalms2_C\",\"PosturalSwayAccRangeSagittalms2_C\",\n",
        "\"PosturalSwayAnglesSwayAreaRadiusCoronaldegrees_C\",\"PosturalSwayAngles95EllipseAxis2Radiusdegrees_C\",\n",
        "\"PosturalSwayAnglesSwayAreaRotationdegrees_C\",\"PosturalSwayAnglesSwayAreadegrees2_C\",\n",
        "\"PosturalSwayAnglesDurations_C\",\"PosturalSwayAnglesRMSSwaydegrees_C\",\"PosturalSwayAnglesRMSSwayCoronaldegrees_C\",\n",
        "\"PosturalSwayAnglesRMSSwaySagittaldegrees_C\"]\n",
        "Balance_Var = Condition1 + Condition2 + Condition3 + Condition4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uNRYdEWfDyck"
      },
      "outputs": [],
      "source": [
        "All_Var = Y_Var+Demographic_var+Gait_SUBCATIGEROIES+Balance_Var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF_JryKSDycl"
      },
      "source": [
        "# Classify X and Y s and making lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JXnLySHTDycm"
      },
      "outputs": [],
      "source": [
        "# Classify X \n",
        "X_All = df.drop(Y_Var, axis=1)\n",
        "X_Gait = df.drop(Y_Var + Balance_Var, axis=1)\n",
        "X_Balance = df.drop(Y_Var + Gait_SUBCATIGEROIES, axis=1)\n",
        "X_Balance_C1 = df.drop(Y_Var + Gait_SUBCATIGEROIES + Condition2 + Condition3 + Condition4, axis=1)\n",
        "X_Balance_C2 = df.drop(Y_Var + Gait_SUBCATIGEROIES + Condition1 + Condition3 + Condition4, axis=1)\n",
        "X_Balance_C3 = df.drop(Y_Var + Gait_SUBCATIGEROIES + Condition2 + Condition1 + Condition4, axis=1)\n",
        "X_Balance_C4 = df.drop(Y_Var + Gait_SUBCATIGEROIES + Condition2 + Condition3 + Condition1, axis=1)\n",
        "X_list = [X_All,X_Gait,X_Balance,X_Balance_C1,X_Balance_C2,X_Balance_C3,X_Balance_C4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oVtNDD0kDycn"
      },
      "outputs": [],
      "source": [
        "# Classify Y\n",
        "Y_Class_Var = ['CurrentPOMSdepressionClasses']\n",
        "Y_POMSD = df['CurrentPOMSdepressionClasses']\n",
        "Y_list = [Y_POMSD]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3orE3RKADyco"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zuCroGl4Dyco"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------\n",
        "# import necessary libraries\n",
        "#----------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.datasets import make_classification\n",
        "#----------------------------------------------------\n",
        "# Import Classifiers from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "#----------------------------------------------------\n",
        "# Import Regressors from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "#----------------------------------------------------\n",
        "# Import model metrics from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import zero_one_loss\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import scipy.stats as st\n",
        "#----------------------------------------------------\n",
        "# Import Confusion matrix from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "#----------------------------------------------------\n",
        "# Import ROC curve from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "#----------------------------------------------------\n",
        "# Import Model evaluation metrics from libraries\n",
        "#----------------------------------------------------\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5NECnvODycq"
      },
      "source": [
        "# Running Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BeZio82_Dycq"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------\n",
        "# Classifier Models\n",
        "#----------------------------------------------------\n",
        "RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini', random_state=33) #criterion can be also : entropy \n",
        "MLPClassifierModel = MLPClassifier(activation='tanh', solver='lbfgs',  learning_rate='constant', early_stopping= False, alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)\n",
        "DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',random_state=33) \n",
        "SVCModel = SVC(kernel= 'rbf', probability=True, C=1.0,gamma='auto')\n",
        "KNNClassifierModel = KNeighborsClassifier(n_neighbors= 5,weights ='uniform', algorithm='auto') \n",
        "GaussianNBModel = GaussianNB()\n",
        "LDAModel = LinearDiscriminantAnalysis()\n",
        "GradientBoostingClassifierModel = GradientBoostingClassifier( learning_rate=1.0, random_state=33) \n",
        "BaggingClassifierModel = BaggingClassifier(base_estimator=SVC(),  random_state=33)\n",
        "#----------------------------------------------------\n",
        "# Clasifier Models list\n",
        "#----------------------------------------------------\n",
        "Class_Model_list = [RandomForestClassifierModel, MLPClassifierModel, DecisionTreeClassifierModel, SVCModel,\n",
        "KNNClassifierModel, GaussianNBModel, LDAModel, GradientBoostingClassifierModel, BaggingClassifierModel]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VLm5ErADycr",
        "outputId": "14ab5468-35b1-4cb6-99b6-19a4f436376e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        }
      ],
      "source": [
        "#----------------------------------------------------\n",
        "# Defining Variables that inserted in the code\n",
        "#----------------------------------------------------\n",
        "Y_Class_Var_T = Y_Class_Var\n",
        "X_list_T = X_list\n",
        "Class_Model_list_T = Class_Model_list\n",
        "Top_feature_number = 12\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats= 10, random_state= 33)\n",
        "#----------------------------------------------------\n",
        "# Defining Empty dataframe for results\n",
        "#----------------------------------------------------\n",
        "results_df_class =pd.DataFrame(columns=['y','X_used','X_shape','feature_Selection_method','important_features_list',\n",
        "'important_feature_importances', 'Model_used', 'Model_used', 'Modelcheck_mean','Modelcheck_CI_testscore', \n",
        "'Modelcheck_min', 'Modelcheck_Q1', 'Modelcheck_Q2','Modelcheck_Q3','Modelcheck_max', 'roc_F1_score_mean_score',\n",
        "'roc_auc_mean_score', 'roc_percesion_mean_score','roc_sensetivity_mean_score','negative_log_likelihood','neg_log_loss'])\n",
        "#----------------------------------------------------\n",
        "# Creating for loop for y_name_list\n",
        "#----------------------------------------------------\n",
        "for y_name in Y_Class_Var_T:\n",
        "    y = df[y_name].values \n",
        "    for X in X_list_T:\n",
        "        X_columns_names = X.columns\n",
        "        if('GaitJointBackRightLat.BendMax.degreesmean'in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns):\n",
        "            X_used = \"Whole gait and Balance conditions\"\n",
        "        elif('GaitJointBackRightLat.BendMax.degreesmean'in X.columns):\n",
        "            X_used = \"Gait\"\n",
        "        elif ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns):\n",
        "            X_used = \"Whole Balance conditions\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2' in X.columns:\n",
        "            X_used = \"Condition1\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns:\n",
        "            X_used = \"Condition2\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns:\n",
        "            X_used = \"Condition3\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns:\n",
        "            X_used = \"Condition4\"\n",
        "        for Model_name in Class_Model_list_T:\n",
        "            # Test and Train separation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
        "            # Feature selection application\n",
        "            Model_name.fit(X, y)\n",
        "            # Get importance weights\n",
        "            from sklearn.inspection import permutation_importance\n",
        "            Model_name.score(X_test, y_test)       \n",
        "            # IF statement\n",
        "            importance = permutation_importance (Model_name, X_test, y_test, n_repeats=5, random_state=33)\n",
        "            importance = importance.importances_mean\n",
        "            feature_Selection_method = \"permutation\"\n",
        "            # Get features names with their weights\n",
        "            feats = {} \n",
        "            for feature, importance in zip(X_columns_names, importance):\n",
        "                feats[feature] = importance  \n",
        "            importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
        "            # Sort features with top 12 important feature at the top\n",
        "            importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(Top_feature_number)\n",
        "            # Create feature names list\n",
        "            important_features_list = importanc_df.index.tolist()\n",
        "            important_feature_importances = importanc_df.importance.tolist()\n",
        "            # Create important features dataframe\n",
        "            important_features_df = pd.DataFrame()\n",
        "            # Insert values in dataframe\n",
        "            for K in important_features_list:\n",
        "                important_features_df = important_features_df.append(df[K])\n",
        "            important_features_df = important_features_df.transpose()\n",
        "            XIF = important_features_df\n",
        "            #XIF = important_features_df.values\n",
        "            # Make X list\n",
        "            X_list_int = [X, XIF]\n",
        "            for Xroll in X_list_int:\n",
        "                X_shape = Xroll.shape\n",
        "                #Splitting data\n",
        "                X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
        "                # K-Fold method method\n",
        "                CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
        "                Modelcheck_trainscore = CrossValidateValues1['train_score']\n",
        "                Modelcheck_testscore = CrossValidateValues1['test_score'] \n",
        "                Modelcheck_mean_testscore = CrossValidateValues1['test_score'].mean() \n",
        "                Modelcheck_CI_testscore = st.t.interval(alpha=0.95, df=len(Modelcheck_testscore)-1, loc=np.mean(Modelcheck_testscore), scale=st.sem(Modelcheck_testscore))\n",
        "                Modelcheck_min_testscore = CrossValidateValues1['test_score'].min() \n",
        "                Modelcheck_max_testscore = CrossValidateValues1['test_score'].max() \n",
        "                test_score_Q1 = np.quantile(Modelcheck_testscore, .25)\n",
        "                test_score_Q2 = np.quantile(Modelcheck_testscore, .50)\n",
        "                test_score_Q3 = np.quantile(Modelcheck_testscore, .75)\n",
        "                Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
        "                roc_F1_score_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"f1_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                roc_auc_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"roc_auc_ovo\", cv = cv, n_jobs= -1).mean()\n",
        "                roc_percesion_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"precision_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                roc_sensetivity_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"recall_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                negative_log_likelihood1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_brier_score\", cv = cv, n_jobs= -1)\n",
        "                neg_log_loss1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_log_loss\", cv = cv, n_jobs= -1)\n",
        "                neg_log_loss = np.median(neg_log_loss1)\n",
        "                negative_log_likelihood = np.median(negative_log_likelihood1)\n",
        "                # Insert results in dataframe\n",
        "                results_df_class = results_df_class.append({'X_shape': X_shape,'y': y_name,'X_used': X_used,'important_features_list': important_features_list, \n",
        "                'important_feature_importances':important_feature_importances, 'Model_used':Model_name, 'feature_Selection_method': feature_Selection_method,           \n",
        "                'Modelcheck_mean': Modelcheck_mean_testscore,'Modelcheck_CI_testscore' : Modelcheck_CI_testscore, \n",
        "                'Modelcheck_min': Modelcheck_min_testscore, 'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,\n",
        "                'Modelcheck_Q3': test_score_Q3,'Modelcheck_max': Modelcheck_max_testscore, \n",
        "                'roc_F1_score_mean_score': roc_F1_score_mean_score,'roc_auc_mean_score': roc_auc_mean_score, \n",
        "                'roc_percesion_mean_score': roc_percesion_mean_score,'roc_sensetivity_mean_score': roc_sensetivity_mean_score,\n",
        "                'negative_log_likelihood': negative_log_likelihood,'neg_log_loss': neg_log_loss}, ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------------\n",
        "# Export the results to the excel\n",
        "#----------------------------------------------------\n",
        "results_df_class.to_excel('results_df_class.xlsx')\n",
        "from google.colab import files\n",
        "files.download('results_df_class.xlsx')"
      ],
      "metadata": {
        "id": "kO81f7bzJJ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATrx8wuqDyct"
      },
      "source": [
        "# For the best classifier model Choosing the best n of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LjPQoIyDyct"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------\n",
        "# Defining Variables that inserted in the code\n",
        "#----------------------------------------------------\n",
        "Y_Class_Var_T = Y_Class_Var\n",
        "X_list_T = [X_list[2]]\n",
        "Class_Model_list_T = Class_Model_list[0]\n",
        "Top_feature_number = 12\n",
        "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats= 2, random_state= 33)\n",
        "#----------------------------------------------------\n",
        "# Defining Empty dataframe for results\n",
        "#----------------------------------------------------\n",
        "results_df_class_Bestn =pd.DataFrame(columns=['y','X_used','X_shape','number','feature_Selection_method','important_features_list',\n",
        "'important_feature_importances', 'Model_used', 'Model_used', 'Modelcheck_mean','Modelcheck_CI_testscore', \n",
        "'Modelcheck_min', 'Modelcheck_Q1', 'Modelcheck_Q2','Modelcheck_Q3','Modelcheck_max', 'roc_F1_score_mean_score',\n",
        "'roc_auc_mean_score', 'roc_percesion_mean_score','roc_sensetivity_mean_score','negative_log_likelihood','neg_log_loss'])\n",
        "#----------------------------------------------------\n",
        "# Creating for loop for y_name_list\n",
        "#----------------------------------------------------\n",
        "for y_name in Y_Class_Var_T:\n",
        "    y = df[y_name].values \n",
        "    for X in X_list_T:\n",
        "        X_columns_names = X.columns\n",
        "        if('GaitJointBackRightLat.BendMax.degreesmean'in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns):\n",
        "            X_used = \"Whole gait and Balance conditions\"\n",
        "        elif('GaitJointBackRightLat.BendMax.degreesmean'in X.columns):\n",
        "            X_used = \"Gait\"\n",
        "        elif ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns) and ('PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns):\n",
        "            X_used = \"Whole Balance conditions\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2' in X.columns:\n",
        "            X_used = \"Condition1\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_A' in X.columns:\n",
        "            X_used = \"Condition2\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_B' in X.columns:\n",
        "            X_used = \"Condition3\"\n",
        "        elif 'PosturalSwayAcc95EllipseAxis1Radiusms2_C' in X.columns:\n",
        "            X_used = \"Condition4\"\n",
        "        for Model_name in Class_Model_list_T:\n",
        "            # Test and Train separation\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=33, shuffle =False)\n",
        "            # Feature selection application\n",
        "            Model_name.fit(X, y)\n",
        "            # Get importance weights\n",
        "            from sklearn.inspection import permutation_importance\n",
        "            Model_name.score(X_test, y_test)       \n",
        "            # IF statement\n",
        "            importance = permutation_importance (Model_name, X_test, y_test, n_repeats=5, random_state=33)\n",
        "            importance = importance.importances_mean\n",
        "            feature_Selection_method = \"permutation\"\n",
        "            # Get features names with their weights\n",
        "            feats = {} \n",
        "            for feature, importance in zip(X_columns_names, importance):\n",
        "                feats[feature] = importance  \n",
        "            importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'importance'})\n",
        "            for number in range (2,Top_feature_number):\n",
        "                # Sort features with top 12 important feature at the top\n",
        "                importanc_df = importances.sort_values(by ='importance' ,ascending=False).head(Top_feature_number)\n",
        "                # Create feature names list\n",
        "                important_features_list = importanc_df.index.tolist()\n",
        "                important_feature_importances = importanc_df.importance.tolist()\n",
        "                # Create important features dataframe\n",
        "                important_features_df = pd.DataFrame()\n",
        "                # Insert values in dataframe\n",
        "                for K in important_features_list:\n",
        "                    important_features_df = important_features_df.append(df[K])\n",
        "                important_features_df = important_features_df.transpose()\n",
        "                XIF = important_features_df\n",
        "                #XIF = important_features_df.values\n",
        "                # Make X list\n",
        "                X_list_int = [XIF]\n",
        "                for Xroll in X_list_int:\n",
        "                    X_shape = Xroll.shape\n",
        "                    #Splitting data\n",
        "                    X_train, X_test, y_train, y_test = train_test_split(Xroll, y, test_size=0.10, random_state=33, shuffle =True)\n",
        "                    # K-Fold method method\n",
        "                    CrossValidateValues1 = cross_validate(Model_name,Xroll,y,cv=cv,return_train_score = True, n_jobs=-1)\n",
        "                    Modelcheck_trainscore = CrossValidateValues1['train_score']\n",
        "                    Modelcheck_testscore = CrossValidateValues1['test_score'] \n",
        "                    Modelcheck_mean_testscore = CrossValidateValues1['test_score'].mean() \n",
        "                    Modelcheck_CI_testscore = st.t.interval(alpha=0.95, df=len(Modelcheck_testscore)-1, loc=np.mean(Modelcheck_testscore), scale=st.sem(Modelcheck_testscore))\n",
        "                    Modelcheck_min_testscore = CrossValidateValues1['test_score'].min() \n",
        "                    Modelcheck_max_testscore = CrossValidateValues1['test_score'].max() \n",
        "                    test_score_Q1 = np.quantile(Modelcheck_testscore, .25)\n",
        "                    test_score_Q2 = np.quantile(Modelcheck_testscore, .50)\n",
        "                    test_score_Q3 = np.quantile(Modelcheck_testscore, .75)\n",
        "                    Modelcheck_fitscore = CrossValidateValues1['fit_time']\n",
        "                    roc_F1_score_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"f1_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                    roc_auc_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"roc_auc_ovo\", cv = cv, n_jobs= -1).mean()\n",
        "                    roc_percesion_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"precision_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                    roc_sensetivity_mean_score = cross_val_score(Model_name, Xroll, y, scoring=\"recall_macro\", cv = cv, n_jobs= -1).mean()\n",
        "                    negative_log_likelihood1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_brier_score\", cv = cv, n_jobs= -1)\n",
        "                    neg_log_loss1 = cross_val_score(Model_name, Xroll, y, scoring=\"neg_log_loss\", cv = cv, n_jobs= -1)\n",
        "                    neg_log_loss = np.median(neg_log_loss1)\n",
        "                    negative_log_likelihood = np.median(negative_log_likelihood1)\n",
        "                    # Insert results in dataframe\n",
        "                    results_df_class_Bestn = results_df_class_Bestn.append({'X_shape': X_shape,'y': y_name,'X_used': X_used,'important_features_list': important_features_list, \n",
        "                    'important_feature_importances':important_feature_importances, 'Model_used':Model_name, \n",
        "                    'number': number,'feature_Selection_method': feature_Selection_method,           \n",
        "                    'Modelcheck_mean': Modelcheck_mean_testscore,'Modelcheck_CI_testscore' : Modelcheck_CI_testscore, \n",
        "                    'Modelcheck_min': Modelcheck_min_testscore, 'Modelcheck_Q1': test_score_Q1, 'Modelcheck_Q2': test_score_Q2,\n",
        "                    'Modelcheck_Q3': test_score_Q3,'Modelcheck_max': Modelcheck_max_testscore, \n",
        "                    'roc_F1_score_mean_score': roc_F1_score_mean_score,'roc_auc_mean_score': roc_auc_mean_score, \n",
        "                    'roc_percesion_mean_score': roc_percesion_mean_score,'roc_sensetivity_mean_score': roc_sensetivity_mean_score,\n",
        "                    'negative_log_likelihood': negative_log_likelihood,'neg_log_loss': neg_log_loss}, ignore_index=True)\n",
        "#----------------------------------------------------\n",
        "# Export the results to the excel\n",
        "#----------------------------------------------------\n",
        "EXPORT_PATH = r'C:\\Users\\ahmed\\Documents\\Depression_Gait_Balance\\Models\\Model Results\\results_df_class_Bestn.xlsx'\n",
        "results_df_class_Bestn.to_excel(EXPORT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoKZA3P9Dycu"
      },
      "source": [
        "# Visualization of the best Classifier model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQkQ9gtKDycv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "88033ea6e6aaf353f3d26ef69434bb9b1f089d6b00d896155ae24c39a5d92896"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}